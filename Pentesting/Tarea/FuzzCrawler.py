import requests
from bs4 import BeautifulSoup
from email_scraper import scrape_emails

#Objetivo a atacar
target = ""

if (target.endswith("/")):
	print("Se procede a análisis")
else:
	target = target + ("/")
	
def buscadic():
	with open("diccionario.txt") as diccionario:
		return diccionario.readlines()
		
ListaPalabras = buscadic()

for palabra in ListaPalabras:
	response = requests.get(target + palabra.strip(), allow_redirects=False)
	if response.status_code in [200, 301, 202]:
		print("URL = " + target + palabra.strip() + " - Respuesta = ", response.status_code)

#for targets in target:
#	response= requests.get(targets)
#	emails = scrape_emails(response.text)
#Imprimo los resultados
#	print("================= Extracted FIles =============")
#	for email in emails:
#		print(email)
#	print("===============================================")

# Envio peticion de web
response = requests.get(target)

#Creación de la sopa con el parser de HTML
soup = BeautifulSoup(response.text, 'html.parser')
print(soup)

#print(soup)
#soup find all es encontrar todos los elementos a en el html donde haya un href
python_file = open("enlaces.txt", "w")
cont_url = 0
for a in soup.find_all("a", href=True):
	var = (a['href'])
	print("Found the URL:", var if var.startswith("http") else str(response.url[:-1] + var))
	if var.startswith("http"):
		python_file.write(var+'\n')
		cont_url = cont_url + 1
	
print ('Cantidad de URLs encontradas: ' + str(cont_url))
	
python_file.close()